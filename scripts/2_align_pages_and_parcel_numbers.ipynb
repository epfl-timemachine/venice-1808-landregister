{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from functools import reduce \n",
    "import io\n",
    "import sys\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "\n",
    "DHLABSRV_DATA_FOLDER = 'from_dhlabsrv4_iiif/'\n",
    "\n",
    "def capture_output(func, *args, **kwargs):\n",
    "    # Create a StringIO object to capture the output\n",
    "    captured_output = io.StringIO()\n",
    "    # Save the original sys.stdout\n",
    "    original_stdout = sys.stdout\n",
    "    try:\n",
    "        # Redirect sys.stdout to the StringIO object\n",
    "        sys.stdout = captured_output\n",
    "        # Call the function\n",
    "        func(*args, **kwargs)\n",
    "    finally:\n",
    "        # Restore the original sys.stdout\n",
    "        sys.stdout = original_stdout\n",
    "    # Get the captured output as a string\n",
    "    return captured_output.getvalue()\n",
    "\n",
    "# trying to infer the path of the images related to the sommarioni text entries\n",
    "tarf = tarfile.open(join(DHLABSRV_DATA_FOLDER, 'sommmarioni_links_map_iiif_dhlabsrv4.tar.gz'))\n",
    "csv_files = [v for v in capture_output(tarf.list, verbose=False).replace(' ', '').split('\\n') if v.endswith('csv')]\n",
    "f = tarf.extractfile(tarf.getmember(csv_files[0]))\n",
    "all_csv_points = reduce(lambda a,b: pd.concat([a, pd.read_csv(tarf.extractfile(tarf.getmember(b)), names=['parcel_number', 'pages', 'uid1', 'geojson', 'id2'])]), csv_files, pd.DataFrame())\n",
    "all_csv_points['page_name'] = all_csv_points['pages'].str.replace('sommarioni/', '').str.replace('pages/','').str.replace('.json','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "with open(join(DHLABSRV_DATA_FOLDER, 'sommarioni_pages_name.txt'), 'r') as f:\n",
    "    files = [v.replace('\\n','').replace('.jpg', '').replace('./', '') for v in f.readlines()]\n",
    "\n",
    "all_pages = set(files)\n",
    "reference_pages = set(all_csv_points['page_name'].unique())\n",
    "sorted(list(all_pages.difference(reference_pages)))\n",
    "\n",
    "ALL_ORDERED_PAGES = sorted(all_pages)\n",
    "\n",
    "def guess_the_missing_parcel_number_range(page_label:str, reference_list: dict[str, tuple[int,int]]) -> Optional[tuple[int,int]]:\n",
    "    '''\n",
    "    '''\n",
    "    if not page_label in reference_list:\n",
    "        idx = ALL_ORDERED_PAGES.index(page_label)    \n",
    "        if idx == 0 or idx == len(ALL_ORDERED_PAGES) - 1 :\n",
    "            return None\n",
    "        prev_in_list = ALL_ORDERED_PAGES[idx-1]\n",
    "        next_in_list = ALL_ORDERED_PAGES[idx+1]\n",
    "        try:\n",
    "            if prev_in_list in reference_list and next_in_list in reference_list:\n",
    "            # we have a single hole in the list we can fill, let's fill it. \n",
    "                min_range = reference_list[prev_in_list][1] + 1 \n",
    "                max_range = reference_list[next_in_list][0] - 1\n",
    "                return [min_range, max_range]\n",
    "        except Exception as e:\n",
    "            print(page_label, e)\n",
    "        return None  \n",
    "    return None  \n",
    "\n",
    "pages_nmbr = all_csv_points.groupby('page_name').agg(lambda g: (min(g), max(g)))['parcel_number'].to_dict()\n",
    "pn_df = pd.DataFrame(ALL_ORDERED_PAGES, columns=['fn']).set_index('fn')\n",
    "pn_df['truth_pn'] = pages_nmbr\n",
    "pn_df = pn_df.reset_index()\n",
    "pn_df['guess_pn'] = pn_df['fn'].apply(lambda v: guess_the_missing_parcel_number_range(v, pages_nmbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "# a noter, les 5 premières images peuvent être skippées \n",
    "MIN = 1\n",
    "MAX = 15700\n",
    "ref_dict = pn_df.set_index('fn')['truth_pn'].fillna('').to_dict()\n",
    "def get_possible_range_for_img(img_name:str, d:dict) -> tuple[int,int]:\n",
    "\n",
    "    img_name = img_name.replace('.jpg', '')\n",
    "    itms = list(d.items())\n",
    "    if img_name in d:\n",
    "        idx = list(d.keys()).index(img_name)\n",
    "    else:\n",
    "        raise Exception(f'{img_name} not found in the reference set of value')\n",
    "    max = MAX\n",
    "    for i in range(idx, len(d)-1):\n",
    "        hit = itms[i+1][1]\n",
    "        if hit:\n",
    "            max = hit[0] \n",
    "            break\n",
    "    \n",
    "    min = MIN\n",
    "    for i in range(0, idx)[::-1]:\n",
    "        hit = itms[i][1]\n",
    "        if hit:\n",
    "            min = hit[1] # on prend le min du tuple, pour avoir un buffer\n",
    "            break\n",
    "    if min > max:\n",
    "        min, max = MIN, MAX\n",
    "    return (min, max)\n",
    "\n",
    "\n",
    "def fetch_corner_of_image_and_down_sample(img: Image, \n",
    "                                          top_corner:bool = True, \n",
    "                                          crop_factor:float = .33, \n",
    "                                          downsample_factor: float = .5):\n",
    "    \"\"\"\n",
    "    Get the top or bottom corner of an image according to the crop factor (expressed as a fraction of the image width)\n",
    "    Downscale the resulting JPEG image by a certain factor.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to the input JPEG image.\n",
    "        output_path (str): Path to save the downscaled image.\n",
    "        factor (float): Downscaling factor (e.g., 0.5 for half size, 0.25 for quarter size, etc.).\n",
    "    \"\"\"\n",
    "    # Calculate new dimensions\n",
    "    width, height = img.size\n",
    "\n",
    "    crop_w = int(width*crop_factor) \n",
    "\n",
    "    crop_coords = (0, 0, crop_w, crop_w) if top_corner else (0, height-crop_w, crop_w, height)\n",
    "    \n",
    "    new_width = int(crop_w * downsample_factor)\n",
    "    new_width = new_width if new_width > 512 else 512\n",
    "    # Resize the image\n",
    "    resized_img = img.crop(crop_coords).resize((new_width, new_width), Image.LANCZOS)\n",
    "    return resized_img\n",
    "\n",
    " \n",
    "\n",
    "for img in Path(DHLABSRV_DATA_FOLDER).rglob('*.jpg'):\n",
    "    img_obj = Image.open(img)\n",
    "    sample_img1 = fetch_corner_of_image_and_down_sample(img_obj,crop_factor=.5, downsample_factor=.1, top_corner=False) \n",
    "    sample_img2 = fetch_corner_of_image_and_down_sample(img_obj,crop_factor=.5, downsample_factor=.1, top_corner=True) \n",
    "    min, max = get_possible_range_for_img(str(img), ref_dict)\n",
    "    print(img)\n",
    "    print(min, max)\n",
    "    display(sample_img2)\n",
    "    display(sample_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_attempt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "top_corner_identify_prompt = PromptTemplate.from_template(\n",
    "    template='On this scan of an archival document, there is a column with the header reading \"della Mappa\". Knowing they are manually written and in a sequential order, within the range of number from {min} to {max}, what are the values appearing under this colum, reading it top to bottom? Answer 0 in case no values are found, or if a value is out of the proposed range. \\n{format_instructions}',\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "bottom_corner_identify_prompt = PromptTemplate.from_template(\n",
    "    template='On this scan of an archival document, knowing they are manually written and in a sequential order, within the range of number from {min} to {max}, reading from top to bottom, what are the values appearing the left-most column holding numbers? Answer 0 in case no value are found, or if a value is out of the proposed range\\n{format_instructions}',\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "\n",
    "def generate_gpt_img_msg(prompt:str, image:Image) -> list[dict]:\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    return [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                \"detail\": \"low\"\n",
    "            },\n",
    "            },\n",
    "        ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "\n",
    "\n",
    "with open('openaiapikey.txt', 'r') as f: \n",
    "    cont = f.readlines()\n",
    "\n",
    "params = dict((v.replace('\\n','').split('=')) for v in cont)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  openai_organization=params['oraganization_id'],\n",
    "  openai_api_key=params['api_key'],\n",
    "  model='gpt-4o'\n",
    ")\n",
    "\n",
    "# prompt_and_model = prompt | model\n",
    "# output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
    "# parser.invoke(output)\n",
    "\n",
    "\n",
    "t_img = 'from_dhlabsrv4_iiif/0006.jpg'\n",
    "\n",
    "for img in Path(DHLABSRV_DATA_FOLDER).rglob('*.jpg'):\n",
    "    min, max = get_possible_range_for_img(str(img), ref_dict)\n",
    "    img_top = fetch_corner_of_image_and_down_sample(img,crop_factor=.5, downsample_factor=.1, top_corner=True) \n",
    "    msg_top = generate_gpt_img_msg(top_corner_identify_prompt.format(min=min, max=max), img_top)\n",
    "    display(img_top)\n",
    "    print(llm.invoke(msg_top).content)\n",
    "    img_bottom = fetch_corner_of_image_and_down_sample(img,crop_factor=.5, downsample_factor=.1, top_corner=False) \n",
    "    msg_bottom = generate_gpt_img_msg(bottom_corner_identify_prompt.format(min=min, max=max), img_bottom)\n",
    "    display(img_bottom)\n",
    "    print(llm.invoke(msg_bottom).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "ALL_ORDERED_PAGES\n",
    "cache_fp = 'gpt_page_reads_and_res.json'\n",
    "res_dict = {v: None for v in ALL_ORDERED_PAGES if not v.endswith('0001') and not v.endswith('0002')}\n",
    "if os.path.exists(cache_fp):\n",
    "    with open(cache_fp, 'r') as f:\n",
    "        res_dict = json.load(f)\n",
    "\n",
    "client = paramiko.SSHClient()\n",
    "client.load_system_host_keys()\n",
    "client.connect('cdhvm0003.xaas.epfl.ch', username='viaccoz')\n",
    "stdin, stdout, stderr = client.exec_command('ls -l')\n",
    "sftp_client = client.open_sftp()\n",
    "for v in tqdm(res_dict.keys()):\n",
    "    if not res_dict[v]:\n",
    "        try:\n",
    "            \n",
    "            min, max = get_possible_range_for_img(str(v), ref_dict)\n",
    "            remote_file = sftp_client.open(f'/mnt/ltm/data/venice/sommarioni/registry/{v}.jpg')\n",
    "            img = Image.open(remote_file)\n",
    "            img_top = fetch_corner_of_image_and_down_sample(img,crop_factor=.5, downsample_factor=.1, top_corner=True) \n",
    "            msg_top = generate_gpt_img_msg(top_corner_identify_prompt.format(min=min, max=max), img_top)\n",
    "            top_res = llm.invoke(msg_top).content\n",
    "            img_bottom = fetch_corner_of_image_and_down_sample(img,crop_factor=.5, downsample_factor=.1, top_corner=False) \n",
    "            msg_bottom = generate_gpt_img_msg(bottom_corner_identify_prompt.format(min=min, max=max), img_bottom)\n",
    "            bottom_res = llm.invoke(msg_bottom).content\n",
    "            res_dict[v] = (top_res, bottom_res)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            with open(cache_fp, 'w+') as f:\n",
    "                json.dump(res_dict, f, indent=2)\n",
    "            remote_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heurisics in order to validate or not the missing results\n",
    "\n",
    "1. Il y a 40 lignes par page du Sommarioni. Il y a toujours au minimum 2 lignes par entrée, donc Max d'entrée par ligne : 20. Toute pages dont la rangée s'étend à plus que 20 => loin. \n",
    "2. Enlever toutes les entrées qui sont simplement 0 \n",
    "3. enlever les entrées non numérique\n",
    "4. Dès que le min du min range est plus grand que le max du max range => poubelle. \n",
    "    * (4.5) tester aussi max(minr) > min(maxr) comme règle de nettoyage _j'ai testé, c'est trop radical, ça enlève des bons cas ou juste chatgpt a overshoot sans que les min max soient faux_.\n",
    "5. tester que max(maxr) - min(minr) < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "def list_vals_to_list(v):\n",
    "    try:\n",
    "        return literal_eval('['+v+']')\n",
    "    except Exception as e:\n",
    "        print(v)\n",
    "        return None\n",
    "    \n",
    "def has_any_zero(l:list)->bool:\n",
    "    return any([v == 0 for v in l])\n",
    "\n",
    "def infer_missing_range(vals: list[list[int]], step_counter=1) -> list[list[int]]:\n",
    "    vals_to_fill = vals.copy()\n",
    "    # to keep a trace of which values were inferred.\n",
    "    for i in range(1, len(vals)-1):\n",
    "        curr_min = vals[i][1]\n",
    "        try:\n",
    "            prev_max = vals[i-1][2]\n",
    "            next_min = vals[i+1][1]\n",
    "            if np.isnan(curr_min) and prev_max != 0 and next_min != 0:\n",
    "                if prev_max and next_min and prev_max < next_min:\n",
    "                    vals_to_fill[i][1] = prev_max + 1\n",
    "                    vals_to_fill[i][2] = next_min - 1\n",
    "                    vals_to_fill[i][3] = \"By Bounds \"+str(step_counter)\n",
    "        except Exception as e:\n",
    "            print(vals[i], e)\n",
    "    return vals_to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cache_fp = 'gpt_page_reads_and_res.json'\n",
    "\n",
    "with open(cache_fp, 'r') as f:\n",
    "    res_dict = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "#res_dict = {[k,list_vals_to_list(v[0]),list_vals_to_list(v[1])] for k, v in res_dict.items()}\n",
    "page_reads = {}\n",
    "for t,v in res_dict.items():\n",
    "    if type(v) == list:\n",
    "        page_reads[t] = [list_vals_to_list(v[0]),list_vals_to_list(v[1])]\n",
    "    else:\n",
    "        page_reads[t] = None\n",
    "\n",
    "df = pd.DataFrame(columns=['page', 'minr', 'maxr'], data=[[v1, v2[0], v2[1]] for v1, v2 in page_reads.items() if v2])\n",
    "\n",
    "# removing all zeros entries:\n",
    "df_no_zeros = df[df.apply(lambda v: not has_any_zero(v['minr']) and not has_any_zero(v['maxr']) , axis=1)]\n",
    "print('Removing 0 entries:', len(df_no_zeros))\n",
    "df_less_than_20 = df_no_zeros[df_no_zeros.apply(lambda v: len(v['minr'])+len(v['maxr']) < 20, axis=1)]\n",
    "print('Removing more than twenty values:', len(df_less_than_20))\n",
    "df_numeric = df_less_than_20[df_less_than_20.apply(lambda v: all([type(vv) == int for vv in v['minr'] + v['maxr']]), axis=1)]\n",
    "print('Removing entries no numerics:', len(df_numeric))\n",
    "df_extreme_mins = df_numeric[df_numeric.apply(lambda v: min(v['minr']) < max(v['maxr']), axis=1)]\n",
    "print('Removing when the min(minr) > max(maxr):', len(df_extreme_mins))\n",
    "# df_inclusive_mins = df_extreme_mins[df_extreme_mins.apply(lambda v: max(v['minr']) < min(v['maxr']), axis=1)]\n",
    "# print('Removing when the max(minr) > min(maxr):', len(df_inclusive_mins))\n",
    "df_range_less_20 = df_extreme_mins[df_extreme_mins.apply(lambda v: max(v['maxr']) - min(v['minr']) < 20, axis=1)]\n",
    "print('Removing when the max(maxr) - min(minr) < 20:', len(df_range_less_20))\n",
    "df_range_less_20.to_csv('gpt_page_reads_filtered.csv', index=False)\n",
    "\n",
    "df_extreme_vals_less_20 = df_extreme_mins[df_extreme_mins.apply(lambda v: v['maxr'][-1] - v['minr'][0] < 20, axis=1)]\n",
    "print('Instead when the maxr[-1] - minr[0] < 20:', len(df_extreme_vals_less_20))\n",
    "df_filtered = df_extreme_vals_less_20.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing missing page numbers in the case we have max-min values for page n and n+2, but not n+1, \n",
    "#  and that way infer the missing range for them\n",
    "\n",
    "import numpy as np\n",
    "df_filtered['min'] = df_filtered['minr'].apply(lambda v: min(v))\n",
    "df_filtered['max'] = df_filtered['maxr'].apply(lambda v: max(v))\n",
    "\n",
    "df_aug = df[['page']].merge(df_filtered[['page', 'min', 'max']], on='page', how='left')\n",
    "df_aug['inferred'] = \"GPT\"\n",
    "vals = df_aug[['page', 'min', 'max', 'inferred']].values\n",
    "\n",
    "print(len(df_aug[~df_aug['min'].isna()]))\n",
    "df_aug = pd.DataFrame(infer_missing_range(vals), columns=['page', 'min', 'max', 'inferred'])\n",
    "print(len(df_aug[~df_aug['min'].isna()]))\n",
    "df_aug[~df_aug['min'].isna()]\n",
    "df_aug['min'] = df_aug['min'].apply(lambda v: int(v) if v and not np.isnan(v) else None)\n",
    "df_aug['max'] = df_aug['max'].apply(lambda v: int(v) if v and not np.isnan(v) else None)\n",
    "# df_aug.to_csv('gpt_page_reads_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now keeping only the pages without min max that are after OR before a page with the values entered.\n",
    "FMT_val = 'https://image-timemachine.epfl.ch/iiif/3/venice%2Fsommarioni%2Fregistry%2F{reg}%2F{pg}.jpg/full/max/0/default.jpg'\n",
    "def page_range_to_iiif(page_range: tuple[int,int]) -> str:\n",
    "    reg, nmb = page_range.split('/')\n",
    "    return FMT_val.format(reg=reg, pg=nmb)\n",
    "\n",
    "vals_to_fill = []\n",
    "df_inferred = df_aug.copy()\n",
    "for i in range(2, len(df_aug.values)-1):\n",
    "    if np.isnan(df_inferred.iloc[i]['min']):\n",
    "        if not np.isnan(df_inferred.iloc[i-1]['min']):\n",
    "            vals_to_fill.append([df_aug.iloc[i]['page'], page_range_to_iiif(df_aug.iloc[i]['page']), None, None])\n",
    "        elif not np.isnan(df_inferred.iloc[i+1]['min']) and not np.isnan(df_inferred.iloc[i-2]['min']):\n",
    "            vals_to_fill.append([df_aug.iloc[i]['page'], page_range_to_iiif(df_aug.iloc[i]['page']), None, None])\n",
    "            \n",
    "#pd.DataFrame(vals_to_fill, columns=['page', 'iiif_link', 'min', 'max']).to_csv('gpt_page_reads_to_fill.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filled_manually = pd.read_csv('manual_labeling/gpt_page_reads_gap_manual_fills.csv')\n",
    "filled_manually['inferred'] = \"Manual\"\n",
    "filled_manually['min'] = filled_manually['min'].apply(lambda v: float(v) if v.isnumeric() else None)\n",
    "filled_manually['max'] = filled_manually['max'].apply(lambda v: float(v) if v.isnumeric() else None)\n",
    "df_inferred2 = df_inferred.set_index('page')\n",
    "for i, row in filled_manually.iterrows():\n",
    "    df_inferred2.loc[row['page'], 'min'] = row['min']\n",
    "    df_inferred2.loc[row['page'], 'max'] = row['max']\n",
    "    df_inferred2.loc[row['page'], 'inferred'] = row['inferred']\n",
    "\n",
    "print(len(df_inferred2[~df_inferred2['min'].isna()]))\n",
    "df_inferred2 = pd.DataFrame(infer_missing_range(df_inferred2.reset_index().values), columns=['page', 'min', 'max', 'inferred'])\n",
    "print(len(df_inferred2[~df_inferred2['min'].isna()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inferred2['iiif_link'] = df_inferred2['page'].apply(lambda v: page_range_to_iiif(v))\n",
    "df_inferred2['min'] = df_inferred2['min'].apply(lambda v: int(v) if v and not np.isnan(v) else None)\n",
    "df_inferred2['max'] = df_inferred2['max'].apply(lambda v: int(v) if v and not np.isnan(v) else None)\n",
    "# df_inferred2[['iiif_link', 'min', 'max']].to_csv('gpt_page_reads_augmented_manually.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching the second pass and revert the iiif link to reg number\n",
    "df_inferred3 = df_inferred2.copy()\n",
    "df_manual2 = pd.read_csv('manual_labeling/gpt_page_reads_augmented_manually_2nd_pass.csv')\n",
    "df_manual2['page'] = df_manual2['iiif_link'].apply(lambda v: v.split('%2F')[-2] + '/' + v.split('%2F')[-1].split('/')[0].replace('.jpg', ''))\n",
    "\n",
    "df_manual2['min'] = df_manual2['min'].apply(lambda v: float(v))\n",
    "df_manual2['max'] = df_manual2['max'].apply(lambda v: float(v))\n",
    "df_manual2['inferred'] = \"Manual\"\n",
    "df_inferred3 = df_inferred3.set_index('page')\n",
    "for i, row in df_manual2.iterrows():\n",
    "    df_inferred3.loc[row['page'], 'min'] = row['min']\n",
    "    df_inferred3.loc[row['page'], 'max'] = row['max']\n",
    "    df_inferred3.loc[row['page'], 'inferred'] = row['inferred']\n",
    "\n",
    "cols = ['page', 'min', 'max', 'inferred']\n",
    "print(len(df_inferred3[~df_inferred3['min'].isna()]))\n",
    "df_inferred3 = pd.DataFrame(infer_missing_range(df_inferred3.reset_index()[cols].values, step_counter=2), columns=cols)\n",
    "print(len(df_inferred3[~df_inferred3['min'].isna()]))\n",
    "df_inferred3['iiif_link'] = df_inferred3['page'].apply(lambda v: page_range_to_iiif(v))\n",
    "df_inferred3[['iiif_link','page', 'min', 'max', 'inferred']].to_csv('final_manual_pass_on_all_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now cheking all gpt reads that seems out of bounds wrt the closest ones\n",
    "\n",
    "df_3rd_pass = pd.read_csv('manual_labeling/3rd_pass_on_all_data.csv')\n",
    "df_3rd_pass\n",
    "df3gpt = df_3rd_pass[df_3rd_pass['inferred'] == 'GPT']\n",
    "df3gpt\n",
    "gpt_wrong_idx = []\n",
    "for i, row in df3gpt.iterrows():\n",
    "    if i < 2 or i > len(df3gpt) - 2:\n",
    "        continue\n",
    "    cmin, cmax = row['min'], row['max']\n",
    "    prev_max = df_3rd_pass.iloc[i-1]['max']\n",
    "    next_min = df_3rd_pass.iloc[i+1]['min']\n",
    "    if cmin < prev_max or cmax > next_min:\n",
    "        gpt_wrong_idx.append(i)\n",
    "\n",
    "print(len(gpt_wrong_idx))\n",
    "# df3gpt.iloc[gpt_wrong_idx].to_csv('wrong_gpt_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_3rd_pass.copy()\n",
    "df_all = df_all.set_index('page')\n",
    "df_4th_pass = pd.read_csv('manual_labeling/4th_pass_wrong_gpt_values.csv')\n",
    "for i, row in df_4th_pass.iterrows():\n",
    "    df_all.loc[row['page'], 'min'] = row['min']\n",
    "    df_all.loc[row['page'], 'max'] = row['max']\n",
    "    df_all.loc[row['page'], 'inferred'] = row['inferred']\n",
    "\n",
    "df_all = df_all.reset_index()\n",
    "df_all.to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally. whenever we have the previous max_page == min_page, that means the subparcels are split between two pages,\n",
    "# so we have to manually check on which subparcel the current page starts.\n",
    "\n",
    "df_5th = pd.read_csv('manual_labeling/5th_pass_over_all_data.csv')\n",
    "print(len(df_5th))\n",
    "df_5th = df_5th[df_5th['min'] != \"0.0\"]\n",
    "print(len(df_5th))\n",
    "df_5th['subparcel_start'] = 0.0\n",
    "subparcels_rows_idx = []\n",
    "for i in range(len(df_5th)):\n",
    "    # un exemple ou ça joue: reg4/0076 & 0077\n",
    "    if i < 2 or i > len(df_5th) - 2:\n",
    "        continue\n",
    "    cmin = df_5th.iloc[i]['min']\n",
    "    prev_max = df_5th.iloc[i-1]['max']\n",
    "\n",
    "    if cmin == prev_max:\n",
    "        subparcels_rows_idx.append(i)\n",
    "# df_5th.iloc[subparcels_rows_idx][['page', 'min', 'subparcel_start', 'iiif_link']].to_csv('subparcel_split_1st_pass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('manual_labeling/5th_pass_over_all_data.csv').set_index('page')\n",
    "df_subparcels = pd.read_csv('manual_labeling/subparcel_split_1st_manual_pass.csv')\n",
    "df_subparcels\n",
    "for _, row in df_subparcels.iterrows():\n",
    "    df_final.loc[row['page'], 'min'] = row['min']\n",
    "    df_final.loc[row['page'], 'subparcel_start'] = row['subparcel_start']\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final[df_final['min'] != \"0.0\"].reset_index()\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('manual_labeling/5th_pass_over_all_data.csv')\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def find_page_from_parcel_number(parcel_number:str, subparcel_number:str, df_pages:pd.DataFrame) -> str:\n",
    "    if type(parcel_number) == str and parcel_number.isnumeric():\n",
    "        parcel_number = float(parcel_number)\n",
    "    else:\n",
    "        return None\n",
    "    candidates = []\n",
    "    for i, row in df_pages.iterrows():\n",
    "        try:\n",
    "            if float(row['min']) <= parcel_number and float(row['max']) >= parcel_number:\n",
    "                candidates.append(i)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        print(f'No candidates for {parcel_number}')\n",
    "        return None\n",
    "    if len(candidates) == 1:\n",
    "        return df_pages.iloc[candidates[0]]['page']\n",
    "    if len(candidates) == 2 and candidates[1] - candidates[0] == 1:\n",
    "        # subparcel case\n",
    "        if not subparcel_number:\n",
    "            print(f'Missing subparcel number for {parcel_number}, got two candidates for the value:', [df_pages.iloc[v]['page'] for v in candidates])\n",
    "            return None\n",
    "        if subparcel_number.isnumeric():\n",
    "            subparcel_number = float(subparcel_number) \n",
    "        else:\n",
    "            print(f'Weird subparcel number for {parcel_number}:', subparcel_number)\n",
    "            return None\n",
    "        if subparcel_number < df_pages.iloc[candidates[0]]['subparcel_start']:\n",
    "            return df_pages.iloc[candidates[0]]['page']\n",
    "        else:\n",
    "            return df_pages.iloc[candidates[1]]['page']\n",
    "    else:\n",
    "        # I have to check with Isabella what's the matter with Reg7 parcel numbers\n",
    "        if 'reg1' in df_pages.iloc[candidates[0]]['page']:\n",
    "            return df_pages.iloc[candidates[0]]['page']\n",
    "        print(f'Multiple candidates for {parcel_number}:', [df_pages.iloc[v]['page'] for v in candidates])\n",
    "        return None\n",
    "sommarioni = pd.read_json('../../1808_Sommarioni/sommarioni_text_data_20240709.json')\n",
    "sommarioni['page'] = sommarioni.progress_apply(lambda r: find_page_from_parcel_number(r['parcel_number'], r['sub_parcel_number'], df_final), axis=1)\n",
    "sommarioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "sommarioni[sommarioni['page'].isna()][['parcel_number', 'sub_parcel_number', 'page']].to_csv('sommarioni_pn_not_found.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# finally, matching all the manual work done on the parcel and subparcels:\n",
    "df_matching_pn = pd.read_csv('manual_labeling/sommarioni_pn_manual_reconciliation.csv')\n",
    "sommarioni_fp = sorted(list(Path('../../1808_Sommarioni/').rglob('sommarioni_text_data_with_pages_*.json')))[-1]\n",
    "sommarioni = pd.read_json(sommarioni_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s_matched = sommarioni.copy()\n",
    "for i, row in df_matching_pn.iterrows():\n",
    "    spn = row['sub_parcel_number']\n",
    "    if type(spn) is str:\n",
    "        s_matched.loc[(s_matched['parcel_number'] == row['parcel_number']) & (s_matched['sub_parcel_number'] == spn) , 'page'] = row['page']\n",
    "    else:\n",
    "        s_matched.loc[s_matched['parcel_number'] == row['parcel_number'] , 'page'] = row['page']\n",
    "        \n",
    "# prevented some HRs in the data configuration to be correctly linked to the right canvas id.\n",
    "manual_correction = {\n",
    "    'reg3/205':'reg3/0205',\n",
    "    'reg5/118':'reg5/0118',\n",
    " 'reg6bis/205':'reg6bis/0205',\n",
    " 'reg6bis/206':'reg6bis/0206',\n",
    " 'reg6bis/207':'reg6bis/0207',\n",
    " 'reg6bis/209':'reg6bis/0209',\n",
    " 'reg6bis/211':'reg6bis/0211',\n",
    " 'reg6bis/212':'reg6bis/0212',\n",
    "}\n",
    "s_matched['page'] = s_matched['page'].apply(lambda v: manual_correction[v] if v in manual_correction else v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('entries not matched yet:', s_matched.page.isna().sum())\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def today_date() -> str:\n",
    "    return dt.strftime(dt.today(), '%Y%m%d')\n",
    "txt_file_path = f'../../1808_Sommarioni/sommarioni_text_data_with_pages_{today_date()}.json'\n",
    "\n",
    "# to get rid of utf-8 errors.\n",
    "with open(txt_file_path, 'w', encoding='utf-8') as file:\n",
    "    s_matched.to_json(file, orient='records', indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_matched[s_matched.page.isna()][['place', 'owner','house_number', 'quality', 'parcel_number', 'sub_parcel_number', 'page']].to_csv('sommarioni_pn_not_found_with_owner_place.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
