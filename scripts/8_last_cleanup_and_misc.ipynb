{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('isabella_manual_correction/sommarioni_standardisation_pre_finished_20250514.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the missing class to religious entities and grand schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# owner_standardised_class\n",
    "without_class_ent = df[df.owner_standardised_class.isna() & df.owner_wd.notnull()]\n",
    "for i, row in without_class_ent.iterrows():\n",
    "    df.at[i, 'owner_standardised_class'] = \"religious_entities\"\n",
    "\n",
    "# old_entity_standardized_class\n",
    "without_class_old = df[df.old_entity_standardized_class.isna() & df.old_entity_wd.notnull()]\n",
    "sucoal_grande_smdc = df[df.old_entity_standardized == 'scuola grande santa maria della carità']\n",
    "for i, row in sucoal_grande_smdc.iterrows():\n",
    "    df.at[i, 'old_entity_standardized_class'] = \"scuole_grandi_entities\"\n",
    "\n",
    "for i, row in without_class_old.iterrows():\n",
    "    df.at[i, 'old_entity_standardized_class'] = \"religious_entities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc. Replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(to_replace=\"basilica di santa maria gloriosa dei frari\", value='santa maria gloriosa dei frari', inplace=True)\n",
    "qualities_replace_d = {\n",
    "    \"MAGAZZENO\": \"MAGAZZINO\",\n",
    "    \"CORTO\":\"CORTE\",\n",
    "    \"SCALLA\":\"SCALA\"\n",
    "}\n",
    "\n",
    "def quick_replace(x):\n",
    "    if not pd.isna(x):\n",
    "        for k, v in qualities_replace_d.items():\n",
    "            x = x.replace(k, v)\n",
    "    return x\n",
    "\n",
    "df.qualities = df.qualities.apply(quick_replace)\n",
    "df.owner_type = df.owner_type.apply(lambda x: x.replace(\"LAICO\", \"SECULAR\") if not pd.isna(x) else x)\n",
    "# wrongly classified as \"SCUOLA\"\n",
    "df.at[df[df.parcel_number == '14'].index[0], 'ownership_types'] = ['PUBBLICO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final formatting, removing unwanted columns and producing aggregated versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing the normalized values in qualities (when doing manual correction, sometimes the quotes were not closed or different)\n",
    "def fix_std_list(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace(\"'\", '').replace('\"', '').replace(\"[\", '').replace(\"]\", '')\n",
    "        return [v.strip() for v in x.split(\",\")]\n",
    "    return x\n",
    "\n",
    "df['qualities'] = df['qualities'].apply(fix_std_list)\n",
    "df['ownership_types'] = df['ownership_types'].apply(fix_std_list)\n",
    "\n",
    "\n",
    "df.at[df[df.unique_id == 23518].index[0], 'qualities'] = ['SCUOLA']\n",
    "# df.at[df[df.unique_id == 23443].index[0], 'qualities'] = \"['LUOGO', 'SCUOLA', 'CAMPANILE']\"\n",
    "df.at[df[df.unique_id == 23476].index[0], 'qualities'] = ['SCUOLA']\n",
    "df.at[df[df.unique_id == 23487].index[0], 'qualities'] = ['SCUOLA']\n",
    "df.at[df[df.unique_id == 23491].index[0], 'qualities'] = ['SCUOLA']\n",
    "\n",
    "df['qualities'] = df.qualities.apply(lambda vs: [v.replace(\"RAFFINARIA\", 'RAFFINERIA').replace(\"SAGRESTIA\",'SACRESTIA') for v in vs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "sommarioni_p = Path('../../1808_Sommarioni/')\n",
    "sommarioni_geo_fp = list(sommarioni_p.rglob('sommarioni_geometries_202*.geojson'))[0]\n",
    "gdf = gpd.read_file(sommarioni_geo_fp)\n",
    "gdf['geometry_id'] = gdf['geometry_id'].apply(lambda v: None if v == -1 else str(int(v)))\n",
    "gdf = gdf.rename({'parcel_type': 'geometry_type', 'parish_standardized': 'parish_standardised'}, axis=1)\n",
    "\n",
    "# EPSG:4326 (WGS 84) is in degrees, EPSG:3857 (Web Mercator) is in meters.\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "# adding the area in square meters of the geometries directly in the gdf instead in the registry.\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "value_to_patch = gdf[gdf['parcel_number'] == '3607 1/2'].index[0]\n",
    "# because of a subparcel number which is not referenced in the textual entries, we need to patch the geometry id of the parcel 3607 1/2 to the one of the parcel 3607\n",
    "gdf.at[value_to_patch,'geometry_id'] = \"8625\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporation & Homogenisation of the (yet again) new transcriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  fixing an issue where in the old transcription, some parcel numbers with starting with \"K\" were actually\n",
    "# parcels starting with \"R\", removing them to avoid collision with the actual \"K\" parcels\n",
    "removed_idx = df[df['parcel_number'].str.startswith('K') & ~df.new_transcription].index\n",
    "df = df.drop(removed_idx)\n",
    "\n",
    "df['owner_standardised'] = df['owner_standardised'].apply(lambda s: s.replace('basilica di san pietro di castello', 'chiesa di san pietro di castello') if isinstance(s, str) else s)\n",
    "\n",
    "# adding the missing transcriptions\n",
    "dfp = pd.read_excel('isabella_manual_correction/lacking_transcriptions_20250527.xlsx')\n",
    "\n",
    "# filter out rows with parcel number that have already been treated.\n",
    "dfp = dfp[~dfp['parcel_number'].apply(lambda v: v.startswith('K') if isinstance(v, str) else False)]  \n",
    "# irrévocablement pas dans le registre: 4273\n",
    "dfp = dfp.rename(columns={'LINK': 'page'})\n",
    "dfp['qualities'] = dfp['qualities'].apply(lambda x: [v.strip().upper() for v in x.split(',') if v] if not pd.isna(x) else x)  # ensure qualities are lists\n",
    "dfp['ownership_types'] = dfp['ownership_types'].apply(lambda x: [])\n",
    "\n",
    "def find_geometry_id(v, pn):\n",
    "    if pd.isna(v):\n",
    "        if isinstance(pn, str):\n",
    "            if 'bis' in pn:\n",
    "                # if the parcel number has 'bis', we need to remove it to find the geometry id\n",
    "                pn = pn.replace(' bis', '')\n",
    "            pn = pn.strip()\n",
    "        elif isinstance(pn, float) or isinstance(pn, int):\n",
    "            pn = str(int(pn)).strip()\n",
    "        # find the geometry id based on parcel_number\n",
    "        matching_gdf = gdf[gdf['parcel_number'] == pn]\n",
    "        if not matching_gdf.empty:\n",
    "            return str(int(matching_gdf.iloc[0]['geometry_id']))\n",
    "    return v\n",
    "\n",
    "dfp['geometry_id'] = dfp.apply(lambda r: find_geometry_id(r['geometry_id'], r['parcel_number']), axis=1)\n",
    "\n",
    "# last remaining geometry_id to manually patch. (the vectorized geometry did not have the parcel number transcribed.)\n",
    "dfp.at[dfp[dfp['parcel_number'] == 'AI'].index[0], 'geometry_id'] = 16827\n",
    "# need to find the districts appartenance to the new transcriptions\n",
    "dfp['parish_standardised'] = dfp['geometry_id'].apply(lambda v: gdf[gdf['geometry_id'] == v]['parish_standardised'].values[0] if isinstance(v, str) else None)\n",
    "dfp['parish_standardised'].value_counts()\n",
    "\n",
    "parish_to_district_accronym = {\n",
    "    \"Santa Maria Formosa\": \"NCS\",   \n",
    "    \"Santa Giustina\": \"NCS\",\n",
    "    \"San Pietro di Castello\": \"NCS\",  \n",
    "    \"Santa Ternita\": \"NCS\",     \n",
    "    \"San Martin\": \"NCS\",         \n",
    "    \"San Giovanni in Bragora\": \"NCS\",\n",
    "    \"San Marco\": \"NSM\",            \n",
    "    \"Sant'Antonino\": \"NCS\",     \n",
    "    \"San Ziminian\" : \"NSM\",            \n",
    "    \"San Lio\": \"NCS\",               \n",
    "    \"San Giovanni Elmosinario\": \"NSP\",\n",
    "    \"San Severo\": \"NSP\",     \n",
    "    \"San Provolo\": \"NCS\",             \n",
    "    \"Santa Maria Nova\": \"NCN\",       \n",
    "    \"Santa Marina\": \"NCS\",   \n",
    "    \"San Giovanni Grisostomo\": \"NCN\",\n",
    "    \"San Giovanni Nuovo\": \"NCS\",     \n",
    "    \"San Cancian\": \"NCN\",          \n",
    "    \"San Marcilian\": \"NCN\"          \n",
    "}\n",
    "dfp['district_acronym'] = dfp['parish_standardised'].apply(lambda v: parish_to_district_accronym.get(v, None))\n",
    "# adding \"NCS\" as district_acronym to the row with parcel number \"CA\"\n",
    "dfp.at[dfp[dfp['parcel_number'] == 'CA'].index[0], 'district_acronym'] = \"NCS\"\n",
    "dfp.at[dfp[dfp['parcel_number'] == 'AI'].index[0], 'district_acronym'] = \"NCS\"\n",
    "dfp.at[dfp[dfp['parcel_number'] == 'DR'].index[0], 'district_acronym'] = \"NCS\"\n",
    "dfp = dfp.drop(columns=['geometry_type', 'id', 'parish_standardised'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the unique id to the new transcriptions, to make the uuid already generated the same, we just add the new transcription to the range\n",
    "max_unique_id = int(df.unique_id.max())\n",
    "dfp['unique_id'] = range(max_unique_id, max_unique_id + len(dfp))\n",
    "dfp['new_transcription'] = True\n",
    "cols_not_in_new_transc = [\n",
    "    \"austrian_cadaster_correspondance\",\n",
    "    \"austro_italian_cadaster_correspondance\",\n",
    "    \"is_people\",\n",
    "    \"llm_guess\",\n",
    "    \"area\"\n",
    "]\n",
    "for col in cols_not_in_new_transc:\n",
    "    dfp[col] = None\n",
    "    \n",
    "dfp['qualities'] = dfp['qualities'].apply(lambda v: [] if not isinstance(v, list) else v)\n",
    "df = pd.concat([df, dfp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['owner_standardised'] = df['owner_standardised'].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "df['old_entity_standardized'] = df['old_entity_standardized'].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "\n",
    "replace_vals = {\n",
    "    \"ent_SCR\": 'social_care_entities',\n",
    "    \"ent_SCL_MST\": 'scuole_mestieri_entities',\n",
    "    \"ent_SCL_REL\": 'scuole_religious_entities',\n",
    "    \"ent_REL_UNL\": 'religious_entities', #UNL is for unlikable, but we have found wikidata for most of those, so not applicable. \n",
    "    \"ent_JEW\": 'jew_entities',\n",
    "    \"ent_VNZ\": 'venezia_entities',\n",
    "    \"religious_titles_entities\": 'religious_entities',\n",
    "}\n",
    "\n",
    "df['old_entity_standardized_class'] = df['old_entity_standardized_class'].apply(lambda x: replace_vals[x] if x in replace_vals.keys() else x)\n",
    "df['owner_standardised_class'] = df['owner_standardised_class'].apply(lambda x: replace_vals[x] if x in replace_vals.keys() else x)\n",
    "\n",
    "\n",
    "rename_cols = {\n",
    "    \"old_entity_standardized\": \"old_entity_standardised\",\n",
    "    \"old_entity_standardized_class\": \"old_entity_standardised_class\",\n",
    "}\n",
    "for k, v in rename_cols.items():\n",
    "    df = df.rename(columns={k: v})\n",
    "\n",
    "district_acronym_d = {\n",
    "    \"CN\": \"Cannaregio\",\n",
    "    \"CS\": \"Castello\",\n",
    "    \"SM\": \"San Marco\",\n",
    "    \"DD\": \"Dorsoduro\",\n",
    "    \"SP\": \"San Polo\",\n",
    "    \"SC\": \"San Croce\",\n",
    "    \"CC\": \"Cannaregio\"\n",
    "}\n",
    "\n",
    "def clean_district_acronym_occurrences(s):\n",
    "    if not pd.isna(s):\n",
    "        for acronym in district_acronym_d.keys():\n",
    "            if acronym in s:\n",
    "                s = s.replace(acronym+' -', \"\").strip()\n",
    "                s = s.replace(acronym+' –', \"\").strip()\n",
    "                s = s.replace(acronym+'-', \"\").strip()\n",
    "    return s\n",
    "\n",
    "df['district'] = df['district_acronym'].apply(lambda s: district_acronym_d[s[1:]])\n",
    "# the geometry id in the gdf is required to be a string as to have null values. \n",
    "df['geometry_id'] = df['geometry_id'].astype(str)\n",
    "df['place'] = df['place'].apply(clean_district_acronym_occurrences)\n",
    "df = df.drop(columns=['district_acronym'])\n",
    "\n",
    "# broadcasting the owner_wd to the owner_standardised, upon manual inspection, some entries were not linked to any wikidata entity, so we are broadcasting those when they hold the same values.\n",
    "chiesa_to_wd_id = df[df['owner_wd'].notnull()][['owner_standardised', 'owner_wd']].drop_duplicates().set_index('owner_standardised')['owner_wd'].to_dict()\n",
    "old_ent_chiesa_to_wd_id = df[df['old_entity_wd'].notnull()][['old_entity_standardised', 'old_entity_wd']].drop_duplicates().set_index('old_entity_standardised')['old_entity_wd'].to_dict()\n",
    "chiesa_to_wd_id = {**chiesa_to_wd_id, **old_ent_chiesa_to_wd_id}\n",
    "chiesa_to_wd_id\n",
    "df['owner_wd'] = df['owner_standardised'].apply(lambda x: chiesa_to_wd_id[x] if x in chiesa_to_wd_id else None)\n",
    "df['old_entity_wd'] = df['old_entity_standardised'].apply(lambda x: chiesa_to_wd_id[x] if x in chiesa_to_wd_id else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translating the standardized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_qualities_dictionnary_for_translation = False\n",
    "stand_cols = [\n",
    "    'owner_right_of_use',\n",
    "    'old_religious_entity_type',\n",
    "    'old_owner_right_of_use',\n",
    "    'qualities',\n",
    "    'ownership_types'\n",
    "]\n",
    "if produce_qualities_dictionnary_for_translation:\n",
    "    all_stand_values_and_cols_of_origin = []\n",
    "    for col in stand_cols:\n",
    "        if type(df[col].iloc[0]) == list:\n",
    "            # if the column is a list, we need to flatten it to get unique values\n",
    "            all_stand_values = set()\n",
    "            for vs in df[col].dropna():\n",
    "                all_stand_values.update(vs)\n",
    "        else:\n",
    "            all_stand_values = df[col].dropna().unique()\n",
    "        all_stand_values_and_cols_of_origin = all_stand_values_and_cols_of_origin + [[col, v.strip(), ''] for v in all_stand_values if isinstance(v, str) and v]\n",
    "    pd.DataFrame(all_stand_values_and_cols_of_origin, columns=['column', 'value', 'translation']).to_csv('sommarioni_standard_values_to_translate.csv', index=False, sep=',')\n",
    "\n",
    "dft = pd.read_csv('isabella_manual_correction/sommarioni_standard_values_translated.csv')\n",
    "translation_values = dft.set_index('value')['translation'].to_dict()\n",
    "\n",
    "def translate_value(v):\n",
    "    if isinstance(v, str) and v in translation_values:\n",
    "        return translation_values[v]\n",
    "    if isinstance(v, list):\n",
    "        return [translation_values.get(item, item) for item in v]\n",
    "    else:\n",
    "        return v\n",
    "    \n",
    "for c in stand_cols:\n",
    "    df[c+\"_en\"] = df[c].apply(translate_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fetching back the last manual corrections from Isabella + homgenisation of some fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/qz3rx_bd641gvkmjygtmsgnh0000gp/T/ipykernel_58257/1505467587.py:2: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfn = pd.read_csv('isabella_manual_correction/venice_1808_landregister_textual_entries_internal_20250604.csv', sep=';').drop(columns=['Unnamed: 0'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "Missing vals in owner_standardised_class: 20681\n",
      "Missing vals in owner_standardised_class: 20566\n",
      "Missing vals in old_entity_standardised_class: 22662\n",
      "Missing vals in old_entity_standardised_class: 22656\n",
      "Missing vals in old_religious_entity_type: 23246\n",
      "Missing vals in old_religious_entity_type: 23229\n",
      "Missing vals in old_religious_entity_type_en: 23246\n",
      "Missing vals in old_religious_entity_type_en: 23229\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "dfn = pd.read_csv('isabella_manual_correction/venice_1808_landregister_textual_entries_internal_20250604.csv', sep=';').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "replace_own_stand = {\n",
    "    \"basilica di san marco\": \"chiesa di san marco\",\n",
    "    \"basilica di san pietro di castello\": \"chiesa di san pietro di castello\",\n",
    "    \"basilica di santa maria della salute\": \"santa maria della salute\",\n",
    "}\n",
    "t_val = 'basilica di san pietro di castello'\n",
    "print((dfn['old_entity_standardised'] == t_val).sum())\n",
    "dfn['owner_standardised'] = dfn['owner_standardised'].apply(lambda x: replace_own_stand.get(x,x))\n",
    "dfn['old_entity_standardised'] = dfn['old_entity_standardised'].apply(lambda x: replace_own_stand.get(x,x))\n",
    "print((dfn['old_entity_standardised'] == t_val).sum())\n",
    "\n",
    "def broadcast_owner_value_to_standard_cols(df:pd.DataFrame, standardised_col:str, old_ent_standardised_col:str, see_change:bool = True) -> pd.DataFrame:\n",
    "    # broadcasting the owner_wd to the owner_standardised, upon manual inspection, some entries were not linked to any wikidata entity, so we are broadcasting those when they hold the same values.\n",
    "    owner_to_std_id = df[df[standardised_col].notnull()][['owner_standardised', standardised_col]].drop_duplicates().set_index('owner_standardised')[standardised_col].to_dict()\n",
    "    old_ent_to_std_id = df[df[old_ent_standardised_col].notnull()][['old_entity_standardised', old_ent_standardised_col]].drop_duplicates().set_index('old_entity_standardised')[old_ent_standardised_col].to_dict()\n",
    "    std_dict = {**owner_to_std_id, **old_ent_to_std_id}\n",
    "    print('Missing vals in {}: {}'.format(standardised_col, df[df[standardised_col].isna()].shape[0])) if see_change else None\n",
    "    df[standardised_col] = df['owner_standardised'].apply(lambda x: std_dict.get(x, None))\n",
    "    print('Missing vals in {}: {}'.format(standardised_col, df[df[standardised_col].isna()].shape[0]))if see_change else None\n",
    "    print('Missing vals in {}: {}'.format(old_ent_standardised_col, df[df[old_ent_standardised_col].isna()].shape[0])) if see_change else None\n",
    "    df[old_ent_standardised_col] = df['old_entity_standardised'].apply(lambda x: std_dict.get(x, None))\n",
    "    print('Missing vals in {}: {}'.format(old_ent_standardised_col, df[df[old_ent_standardised_col].isna()].shape[0])) if see_change else None\n",
    "    return df\n",
    "\n",
    "def broadcast_owner_value_to_single_col(df:pd.DataFrame, col:str, standardised_col:str) -> pd.DataFrame:\n",
    "    print('Missing vals in {}: {}'.format(standardised_col, df[df[standardised_col].isna()].shape[0]))\n",
    "    # broadcasting the owner_wd to the owner_standardised, upon manual inspection, some entries were not linked to any wikidata entity, so we are broadcasting those when they hold the same values.\n",
    "    col_to_std_id = df[df[standardised_col].notnull()][[col, standardised_col]].drop_duplicates().set_index(col)[standardised_col].to_dict()\n",
    "    df[standardised_col] = df[col].apply(lambda x: col_to_std_id.get(x, None))\n",
    "    print('Missing vals in {}: {}'.format(standardised_col, df[df[standardised_col].isna()].shape[0]))\n",
    "    return df\n",
    "\n",
    "dfn = broadcast_owner_value_to_standard_cols(dfn, 'owner_standardised_class', 'old_entity_standardised_class')\n",
    "dfn = broadcast_owner_value_to_single_col(dfn, 'old_entity_standardised', 'old_religious_entity_type')\n",
    "dfn = broadcast_owner_value_to_single_col(dfn, 'old_entity_standardised', 'old_religious_entity_type_en')\n",
    "\n",
    "dfn = dfn.rename(columns={'owner': 'owner_transcription'})\n",
    "def quick_reg_format(x):\n",
    "    if isinstance(x, str):\n",
    "        reg, numb = x.split('/')\n",
    "        numb = int(numb.strip())\n",
    "        reg = reg.replace('reg', 'Reg ')\n",
    "        return f'{reg}, pg. {numb}'\n",
    "    return x\n",
    "\n",
    "dfn['citation'] = dfn['page'].apply(lambda x: ('ASVe, Censo Stabile, Catasto Napoleonico, Sommarioni, ' + quick_reg_format(x)) if isinstance(x, str) else x)\n",
    "\n",
    "dfn['ownership_types'] = dfn['ownership_types'].apply(lambda v: literal_eval(v) if isinstance(v, str) else v)\n",
    "dfn['ownership_types_en'] = dfn['ownership_types_en'].apply(lambda v: literal_eval(v) if isinstance(v, str) else v)\n",
    "\n",
    "## Removing \"COMUNE\" from the ownnership_types, as it is not a valid ownership type. (similar information is already in the owner_standardised_class or owner_type)\n",
    "dfn['ownership_types'] = dfn['ownership_types'].apply(lambda x: [v for v in x if v.upper() != 'COMUNE' and v.upper() != 'COMMUNE'] if isinstance(x, list) else x)\n",
    "dfn['ownership_types_en'] = dfn['ownership_types_en'].apply(lambda x: [v for v in x if v.upper() != 'COMUNE' and v.upper() != 'COMMUNE'] if isinstance(x, list) else x)\n",
    "\n",
    "dfn['qualities'] = dfn['qualities'].apply(lambda v: literal_eval(v) if isinstance(v, str) else v)\n",
    "dfn['qualities_en'] = dfn['qualities_en'].apply(lambda v: literal_eval(v) if isinstance(v, str) else v)\n",
    "\n",
    "dfn['old_religious_entity_type'] = dfn['old_religious_entity_type'].apply(lambda s: s.upper() if isinstance(s, str) else s)\n",
    "dfn['old_religious_entity_type_en'] = dfn['old_religious_entity_type_en'].apply(lambda s: s.upper() if isinstance(s, str) else s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New modifications to be made (23.06.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'', 'COMMUNE', 'OWN', 'PUBLIC', 'RENT'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "reduce(lambda a, b: a.union(set(b)) if b else a, df['ownership_types'].tolist(),set())\n",
    "\n",
    "reduce(lambda a, b: a.union(set(b)) if b else a, df['ownership_types_en'].tolist(),set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because changing it in the translated alues doesn't work, at this step of the pipeline, we're basing the latest version of the data on Isabella's latest manual corrections.\n",
    "special_qualities_replace = {\n",
    "    \"HOME\": \"HOUSE\",\n",
    "    \"PLACES\": \"ADJACENT AREAS\",\n",
    "    \"PLACE\": \"ADJACENT AREA\"\n",
    "}\n",
    "\n",
    "def replace_special_en_qualities(ownership_types: list[str]) -> list[str]:\n",
    "    if not isinstance(ownership_types, list):\n",
    "        return ownership_types\n",
    "    return [special_qualities_replace.get(v, v) for v in ownership_types]\n",
    "\n",
    "def add_value_if_not_present(lst, val) -> list[str]:\n",
    "    if not val in lst:\n",
    "        return lst + [val]\n",
    "    return lst\n",
    "\n",
    "def add_public_to_ownership_types_when_venezia_entities_in_own_standardised_class(own_types: list[str], own_types_en: list[str], own_std_class: str) -> tuple[list[str], list[str]]:\n",
    "    if not isinstance(own_types, list):\n",
    "        return own_types, own_types_en\n",
    "    if own_std_class == 'venezia_entities':\n",
    "        if len(own_types) == 1 and own_types[0] == '':\n",
    "            return ['PUBBLICO'], ['PUBLIC']\n",
    "        else:\n",
    "            return add_value_if_not_present(own_types, 'PUBBLICO'), add_value_if_not_present(own_types_en, 'PUBLIC')\n",
    "    else:\n",
    "        return own_types, own_types_en\n",
    "\n",
    "dfn['ownership_types'], dfn['ownership_types_en'] = zip(*dfn.apply(lambda r:\\\n",
    "                                                            add_public_to_ownership_types_when_venezia_entities_in_own_standardised_class(r['ownership_types'], r['ownership_types_en'], r['owner_standardised_class']), axis=1))\n",
    "\n",
    "dfn['qualities_en'] = dfn['qualities_en'].apply(replace_special_en_qualities)\n",
    "dfn['owner_type'] = dfn['owner_type'].apply(lambda v: v.strip().replace('ISTITUZIONE PUBBLICA','SECULAR') if isinstance(v, str) else v)\n",
    "\n",
    "dfn['old_religious_entity_type'] = dfn['old_religious_entity_type'].apply(lambda v: v.strip().replace('ISTITUZIONE PUBBLICA','SECULAR') if isinstance(v, str) else v)\n",
    "\n",
    "dfn['old_religious_entity_type'] = dfn['old_religious_entity_type'].apply(lambda v: v.strip().replace('BASILICA, CONVENTO','CONVENTO') if isinstance(v, str) else v)\n",
    "dfn['old_religious_entity_type'] = dfn['old_religious_entity_type'].apply(lambda v: None if v == \"BASILICA\" else v)\n",
    "\n",
    "\n",
    "dfn['old_religious_entity_type_en'] = dfn['old_religious_entity_type_en'].apply(lambda v: v.strip().replace('BASILICA, CONVENT','CONVENT') if isinstance(v, str) else v)\n",
    "dfn['old_religious_entity_type_en'] = dfn['old_religious_entity_type_en'].apply(lambda v: v.strip().replace('MONASTERO','MONASTERY') if isinstance(v, str) else v)\n",
    "dfn['old_religious_entity_type_en'] = dfn['old_religious_entity_type_en'].apply(lambda v: None if v == \"BASILICA\" else v)\n",
    "\n",
    "da_bis_geometry_id = 16869\n",
    "db_bis_geometry_id = 16870\n",
    "da_bis_ids = [\"way/6125\", \"way/6126\", \"way/5319\"]\n",
    "db_bis_ids = [\"way/12213\", \"way/12064\", \"way/12087\", \"way/12088\"]\n",
    "\n",
    "# matching \"way/12213\", \"way/12064\", \"way/12087\", \"way/12088\" to \"DB Bis\" (with a new geometry id)\n",
    "for ids in da_bis_ids:\n",
    "    gdf.at[gdf[gdf['id'] == ids].index[0], 'geometry_id'] = da_bis_geometry_id\n",
    "\n",
    "dfn.at[dfn[dfn['parcel_number'] == 'DA bis'].index[0], 'geometry_id'] = da_bis_geometry_id\n",
    "\n",
    "# matching \"way/6125\", \"way/6126\", \"way/5319\", to \"DA Bis\" (with a new geometry id)\n",
    "for ids in db_bis_ids:\n",
    "    gdf.at[gdf[gdf['id'] == ids].index[0], 'geometry_id'] = db_bis_geometry_id\n",
    "dfn.at[dfn[dfn['parcel_number'] == 'DB bis'].index[0], 'geometry_id'] = db_bis_geometry_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "portionless_vals = [9124, 9125, 9126, 9127, 9128, \n",
    "            9129, 9130, 9131, 9132, 9133, \n",
    "            9134, 9135, 9136, 9137, 9138, \n",
    "            9139, 9140, 9141, 9142, 9143,\n",
    "            9144, 9145, 9146, 9147, 9148,\n",
    "            9149, 9150, 9151, 9172, 9173,\n",
    "            9174, 9175, 9176, 9177, 9178,\n",
    "            9179, 9180, 9181, 9182, 9183,\n",
    "            9184, 9185, 9186, 9192, 9193,\n",
    "            9194, 9195, 9196, 9197, 9198,\n",
    "            9199, 9200, 9201, 9202, 9203, \n",
    "            9204, 9205, \n",
    "            # 9206 # this one is not portionless, it is the one listing all previous parcels.\n",
    "            ]\n",
    "# for the visualisation in observable to work, adding the \"[porzione di casa]\" to the quality of the portionless parcels.\n",
    "for pn in portionless_vals:\n",
    "    idx = dfn[dfn['parcel_number'] == str(pn)].index[0]\n",
    "    curr_quality = dfn.iloc[idx]['quality']\n",
    "    dfn.at[idx, 'quality'] =  (curr_quality + '[porzione di casa]') if isinstance(curr_quality, str) else '[porzione di casa]'\n",
    "\n",
    "\n",
    "def clean_list_values(x):\n",
    "    if isinstance(x, list):\n",
    "        if len(x) == 1 and x[0] == '':\n",
    "            return []\n",
    "    return x\n",
    "\n",
    "dfn['qualities'] = dfn['qualities'].apply(clean_list_values)\n",
    "dfn['qualities_en'] = dfn['qualities_en'].apply(clean_list_values)\n",
    "dfn['ownership_types'] = dfn['ownership_types'].apply(clean_list_values)\n",
    "dfn['ownership_types_en'] = dfn['ownership_types_en'].apply(clean_list_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = dfn.rename(columns={\"page\": \"page_number\"})\n",
    "remove_cols = [\n",
    "    \"austrian_cadaster_correspondance\",\n",
    "    \"austro_italian_cadaster_correspondance\",\n",
    "    \"new_transcription\",\n",
    "    \"page_number\",\n",
    "    \"is_people\",\n",
    "    \"llm_guess\",\n",
    "    \"owner_type_en\", # ended up being the same value as the base column.\n",
    "    \"area\" # computed in the geometries side instead.\n",
    "]\n",
    "dfn['geometry_id'] = dfn['geometry_id'].apply(lambda v: int(v) if not pd.isna(v) else v)\n",
    "# version with all the columns, for internal use, and for timeAtlas integration\n",
    "dfn.to_json('../../1808_Sommarioni/venice_1808_landregister_textual_entries_internal_version.json', orient='records', indent=2, force_ascii=False)\n",
    "df = dfn.drop(columns=remove_cols)\n",
    "# for consistency with the people dataset. \n",
    "df = df.rename(columns = {'unique_id': 'parcel_id'})\n",
    "df.to_json('venice_1808_landregister_textual_entries.json', orient='records', indent=2, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing the final and aggregated versions\n",
    "\n",
    "## Adding back the people to the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Generating a single geojson file with all the data for paul\n",
    "sommarioni_people = pd.read_json('isabella_manual_correction/people_sommarioni_dataset.json').drop(columns=['edited', 'merged_ids'])\n",
    "sommarioni_people['uid'] = sommarioni_people['uid'].astype(int)\n",
    "sommarioni_people['nucleus_uid'] = sommarioni_people['nucleus_uid'].astype(int)\n",
    "sommarioni_people['parcel_array'] = sommarioni_people.parcel_ids.apply(lambda v: literal_eval('[' + v+']'))\n",
    "\n",
    "sommarioni_people = sommarioni_people.replace({'': None})\n",
    "people_duplicated = []\n",
    "for _, r in sommarioni_people.iterrows():\n",
    "    for parcel_id in r.parcel_array:\n",
    "        new_r = r.copy()\n",
    "        new_r['parcel_id'] = parcel_id\n",
    "        people_duplicated.append(new_r)\n",
    "\n",
    "people_flattened = pd.DataFrame(people_duplicated).drop(columns=['parcel_array', 'parcel_ids'])\n",
    "rename_owner_dict = {v:'own_'+v for v in people_flattened.columns.to_list()}\n",
    "people_flattened = people_flattened.rename(columns=rename_owner_dict)\n",
    "sommarioni_people.drop(columns=['parcel_ids']).rename(columns={'parcel_array': 'parcel_ids'}).to_json('venice_1808_landregister_standardised_people.json', orient='records', indent=2, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_id_people_group = {}\n",
    "for g, group_df in people_flattened.groupby('own_parcel_id'):\n",
    "    group_df = group_df.drop(columns=['own_parcel_id'])\n",
    "    parcel_id_people_group[g] = group_df.to_json(orient='records', force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gdf.to_file('venice_1808_landregister_geometries.geojson', driver='GeoJSON')\n",
    "# no differences with the internal version, but for consistency we save it with the same name as the one used in the registry.\n",
    "gdf.to_file('../../1808_Sommarioni/venice_1808_landregister_geometries_internal_version.geojson', driver='GeoJSON')\n",
    "\n",
    "gdf = gdf.drop(columns=['parcel_number'])\n",
    "gdf['geometry_id'] = gdf['geometry_id'].fillna(-1).astype(int)\n",
    "max_idx = gdf['geometry_id'].astype(int).max()\n",
    "\n",
    "# for all geometry_id == -1, replacing them with a new index starting from the max_idx + 1\n",
    "non_identified = gdf[gdf['geometry_id'] == -1]\n",
    "gdf.loc[non_identified.index, 'geometry_id'] = list(range(max_idx + 1, max_idx + 1 + len(gdf[gdf['geometry_id'] == -1])))\n",
    "\n",
    "\n",
    "# this makes it so the non grouped by parcel_number geometries will be put in their own object in the dedicated aggregatted JSON structure. \n",
    "gdf.geometry_id = gdf.geometry_id.astype(int).astype(str)\n",
    "geometries_group = gdf.groupby('geometry_id')\n",
    "geometries_group_dict = geometries_group.apply(lambda x: x.drop(columns=['geometry_id']).to_json()).to_dict()\n",
    "geometries_group_dict = {str(k):v for k,v in geometries_group_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sommarioni_text_people_group = df.groupby('geometry_id')\n",
    "sommarioni_text_people_group_dict = sommarioni_text_people_group.apply(lambda x: x.drop(columns=['geometry_id']).to_json(orient='records')).to_dict()\n",
    "sommarioni_text_people_group_dict = {str(k):v for k,v in sommarioni_text_people_group_dict.items()}\n",
    "new_struct = []\n",
    "for k, v in geometries_group_dict.items():\n",
    "    text = json.loads(sommarioni_text_people_group_dict[k]) if k in sommarioni_text_people_group_dict else []\n",
    "    people = []\n",
    "    if len(text) > 0:\n",
    "        for t in  text:\n",
    "            parcel_id = t['parcel_id']\n",
    "            if parcel_id in parcel_id_people_group:\n",
    "                # if the parcel_id is in the people group, we add the people to the text\n",
    "                people = people + json.loads(parcel_id_people_group[parcel_id])\n",
    "        t.pop('parcel_id', None)\n",
    "    new_struct.append({'geometries': json.loads(v), 'text':text, 'people': people }) \n",
    "\n",
    "with open('venice_1808_landregister_aggregated_data.json', 'w+') as f:\n",
    "    json.dump(new_struct, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
