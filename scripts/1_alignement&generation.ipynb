{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Alignment\n",
    "\n",
    "This notebook aims to produce a reference clean version of the Sommarioni, aggregating and aligning different sources of data relating to the Sommarioni into a single dataset.\n",
    "\n",
    "## Aligning the geometries and standardisation of the parcel type on Carlo's data:\n",
    "\n",
    "Carlo's dataset proposes standardisation of the owner text from the sommarioni, from which a dataset of the people mentionned was built, which is stored in a speparate file. As the index of his version of the sommmarioni is used to link to his work on the individuals, it is a more sound base to use for the project moving on (and until the right data has been correctly deduced from the original scans). Below are the codes used to merge Carlo's dataset with the geometries fetched from the cadaster-interface app, as well as adding missing columns (standardisation of the type of ownership and the quality of each parcel). This notebook should in principle be run after the `cadaster-interface_data_fetch.ipynb` on then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def today_date() -> str:\n",
    "    return dt.strftime(dt.today(), '%Y%m%d')\n",
    "\n",
    "\n",
    "CADASTER_INTERFACE_DATA_FOLDER = 'fetched_from_cadaster-interface_app/'\n",
    "# aligning the geometries with the text data.\n",
    "gdf = gpd.read_file(join(CADASTER_INTERFACE_DATA_FOLDER, 'cadaster_18080_geometries_from_cadaster_inferface.geojson'))\n",
    "gdf['geometry_id'] = gdf.groupby('parcel_number').ngroup().fillna(-1).astype(int)\n",
    "gdf_number = gdf.groupby('parcel_number').first().reset_index()[['geometry_id', 'parcel_number']]\n",
    "sommarioni_std = pd.read_json('../../named_entity_standardisation/1808_Sommarioni_standardised/sommarioni_dataset_STD.json')\n",
    "sommarioni_std_geom = gdf_number.merge(sommarioni_std, left_on='parcel_number', right_on='numero_della_mappa')\n",
    "geometries_correctly_linked = gdf.drop_duplicates().reset_index().drop(columns=['index']).sort_values(by='geometry_id')\n",
    "# the value for this field is alway \"yes\", redundant we can drop it.\n",
    "geometries_correctly_linked.drop(columns=['parcel_number_written'], inplace=True) \n",
    "geometries_correctly_linked.to_file(f'../../1808_Sommarioni/sommarioni_geometries_{today_date()}.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Carlo's data is missing the standardisation of the type of ownership and category of parcels derived from the parcelCategoryText/qualita which was present on the cadaster interface website\n",
    "sommarioni_categories = pd.read_json(join(CADASTER_INTERFACE_DATA_FOLDER, '1808_sommarioni_from_cadaster_interface.json'))[['parcelCategoryText','parcelCategories', 'parcelOwnershipType']]\n",
    "\n",
    "# so the merge can use a non duplicated version of the data (list is not hashable and can be used for dropping duplicates)\n",
    "sommarioni_categories['parcelCategories_str'] = sommarioni_categories['parcelCategories'].apply(str)\n",
    "sommarioni_categories['parcelOwnershipType_str'] = sommarioni_categories['parcelOwnershipType'].apply(str)\n",
    "\n",
    "sommarioni_categories.drop(columns=['parcelCategories', 'parcelOwnershipType'], inplace=True)\n",
    "\n",
    "sommarioni_std_full = sommarioni_std_geom.merge(sommarioni_categories.drop_duplicates(),left_on='qualita', right_on='parcelCategoryText').drop(columns=['parcelCategoryText', 'numero_della_mappa'])\n",
    "\n",
    "sommarioni_std_full = sommarioni_std_full.rename({\n",
    "          'subalterno': 'sub_parcel_number',\n",
    "          'corr_as': 'austrian_cadaster_correspondance',\n",
    "          'corr_ai': 'austro_italian_cadaster_correspondance',\n",
    "          'denom_pezzi_di_terra': 'house_number', \n",
    "          'place_acronym': 'district_acronym',\n",
    "          'possessore':'owner', \n",
    "          'possessore_standardised': 'owner_standardised', \n",
    "          'qualita': 'quality',\n",
    "          'uniqueID': 'unique_id'}, axis=1)\n",
    "\n",
    "#reconverting the categorical data back to strings\n",
    "sommarioni_std_full['ownership_types'] = sommarioni_std_full['parcelOwnershipType_str'].apply(lambda x: [v.strip() for v in ast.literal_eval(x)])\n",
    "sommarioni_std_full['qualities'] = sommarioni_std_full['parcelCategories_str'].apply(lambda x: [v.strip() for v in ast.literal_eval(x)])\n",
    "sommarioni_std_full.drop(columns=['parcelCategories_str', 'parcelOwnershipType_str'], inplace=True)\n",
    "\n",
    "txt_file_path = f'../../1808_Sommarioni/sommarioni_text_data_{today_date()}.json'\n",
    "\n",
    "# to get rid of utf-8 errors.\n",
    "with open(txt_file_path, 'w', encoding='utf-8') as file:\n",
    "    sommarioni_std_full.to_json(file, orient='records', indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating a single geojson file with all the data for paul\n",
    "sommarioni_people = pd.read_json('../../named_entity_standardisation/1808_Sommarioni_standardised/people_sommarioni_dataset.json').drop(columns=['edited', 'merged_ids'])\n",
    "sommarioni_people['uid'] = sommarioni_people['uid'].astype(int)\n",
    "sommarioni_people['nucleus_uid'] = sommarioni_people['nucleus_uid'].astype(int)\n",
    "sommarioni_people['parcel_array'] = sommarioni_people.parcel_ids.apply(lambda v: ast.literal_eval('[' + v+']'))\n",
    "people_duplicated = []\n",
    "for _, r in sommarioni_people.iterrows():\n",
    "    for parcel_id in r.parcel_array:\n",
    "        new_r = r.copy()\n",
    "        new_r['parcel_id'] = parcel_id\n",
    "        people_duplicated.append(new_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_flattened = pd.DataFrame(people_duplicated).drop(columns=['parcel_array', 'parcel_ids'])\n",
    "rename_owner_dict = {v:'own_'+v for v in people_flattened.columns.to_list()}\n",
    "people_flattened = people_flattened.rename(columns=rename_owner_dict)\n",
    "sommarioni_text_people = sommarioni_std_full.merge(people_flattened, left_on='unique_id', right_on='own_parcel_id', how='left').drop(columns=['own_parcel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file_name = f'../../1808_Sommarioni/aggregated/sommarioni_geometries_function_and_people_standardized_{today_date()}.geojson'\n",
    "to_write = geometries_correctly_linked.merge(sommarioni_text_people, on='geometry_id', how='left').rename(columns={'parcel_number_x': 'parcel_number'}).drop(columns='parcel_number_y').to_json(indent=2)\n",
    "with open(final_file_name, 'w', encoding='utf-8') as file:\n",
    "     file.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, just to have the file with the very few changes (identifiers as int, list is a proper list, same field labels as in the aggregated version and removed useless columns) in the production folder\n",
    "\n",
    "sommarioni_people['parcel_ids'] = sommarioni_people['parcel_array']\n",
    "sommarioni_people.drop(columns='parcel_array', inplace=True)\n",
    "sommarioni_people.rename(columns=rename_owner_dict, inplace=True)\n",
    "people_dataset_fp = f'../../1808_Sommarioni/people_sommarioni_dataset_{today_date()}.json'\n",
    "with open(people_dataset_fp, 'w', encoding='utf-8') as file:\n",
    "    sommarioni_people.to_json(file, orient='records', indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning with the parishes from 1740\n",
    "\n",
    "Using the geometries of the parcels that are registered in the Sommmarioni (so the parish value can be backpropagated to the corresponding HR). We check them against the most corresponding parish geometry (meaning the parish that contains the parcel, or that has the highest percentage of the parcel covered in the parish area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_and_geom = geometries_correctly_linked[['parcel_number', 'geometry']].drop_duplicates()\n",
    "pn_and_geom = pn_and_geom[~pn_and_geom.parcel_number.isna()]\n",
    "parishes = gpd.read_file('../../1740_redrawn_parishes_cleaned_wikidata_standardised.geojson')\n",
    "# will be used to \n",
    "ids_geoms_dict = parishes.groupby('id').agg(list)['geometry'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "def match_parcel_to_parish(parcel_geom, parish_df, intersection_rather_than_contain = False):\n",
    "    candidates = []\n",
    "    try:\n",
    "        for _, r in parish_df.iterrows():\n",
    "            if intersection_rather_than_contain:\n",
    "                if r.geometry.intersects(parcel_geom):\n",
    "                    candidates.append(r['id'])\n",
    "            else:\n",
    "                if r.geometry.contains(parcel_geom):\n",
    "                    candidates.append(r['id'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return candidates if len(candidates) > 0 else None\n",
    "    \n",
    "\n",
    "pn_and_geom['containing_parish'] = pn_and_geom['geometry'].progress_apply(lambda g: match_parcel_to_parish(g, parishes))\n",
    "pn_and_geom['intersect_parish'] = pn_and_geom.\\\n",
    "    progress_apply(lambda r: r['containing_parish'] if r['containing_parish'] else match_parcel_to_parish(r['geometry'], parishes, True) , axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_overlap(g, geoms) -> float:\n",
    "    percentage = 0.\n",
    "    for geom in geoms:\n",
    "        percentage += (g.intersection(geom).area/g.area)\n",
    "    return percentage\n",
    "\n",
    "def intersection_selection(parcel_geom, geometry_ids) -> int:\n",
    "    '''\n",
    "    Given a parcel and a list of parish id, return the parish id that covers\n",
    "    the most the area of the parcel. Basically allows to select the most prominent\n",
    "    parish from which an overlapping parcel belongs to.\n",
    "    '''\n",
    "    if geometry_ids:\n",
    "        if len(geometry_ids) > 1:\n",
    "            percentages = [(id, percentage_overlap(parcel_geom, ids_geoms_dict[id])) for id in geometry_ids]\n",
    "            percentages.sort(key=lambda x: x[1], reverse=True)\n",
    "            return percentages[0][0]\n",
    "        elif len(geometry_ids) == 1:\n",
    "            return geometry_ids[0]\n",
    "        \n",
    "    return None\n",
    "    \n",
    "parish_id_to_name = parishes.set_index('id')['NAME'].to_dict()\n",
    "pn_and_geom['parish_id'] = pn_and_geom.apply(lambda r: intersection_selection(r.geometry,r.intersect_parish),axis=1)\n",
    "pn_and_geom['parish_name'] = pn_and_geom['parish_id'].map(parish_id_to_name)\n",
    "geometries_correctly_linked['parish_standardized'] = pn_and_geom['parish_name']\n",
    "geometries_correctly_linked.to_file(f'../../1808_Sommarioni/sommarioni_geometries_{today_date()}.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy way to quality check the parish attribution from the parcels.\n",
    "import folium\n",
    "from tqdm import tqdm\n",
    "\n",
    "geom_id = '34'\n",
    "\n",
    "map2 = folium.Map(location=[45.433, 12.329], tiles=\"CartoDB Positron\", zoom_start=14.6)\n",
    "for _, r in tqdm(pn_and_geom[pn_and_geom.parish == geom_id].iterrows()):\n",
    "        geo_j = gpd.GeoSeries(r[\"geometry\"]).to_json()\n",
    "        geo_j = folium.GeoJson(data=geo_j) #style_function=lambda x: {\"fillColor\": color_dict[r['parcel_type']], 'color':color_dict[r['parcel_type']]}\n",
    "        folium.Popup(r[\"parcel_number\"]).add_to(geo_j)\n",
    "        geo_j.add_to(map2)\n",
    "\n",
    "for _, r in tqdm(parishes[parishes.id == geom_id].iterrows()):\n",
    "        geo_j = gpd.GeoSeries(r[\"geometry\"]).to_json()\n",
    "        geo_j = folium.GeoJson(data=geo_j,  style_function=lambda x: {\"fillColor\": \"#FF0000\"}) #style_function=lambda x: {\"fillColor\": color_dict[r['parcel_type']], 'color':color_dict[r['parcel_type']]}\n",
    "        geo_j.add_to(map2)\n",
    "map2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
